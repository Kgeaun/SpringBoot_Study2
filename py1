import threading
import queue
import requests
from bs4 import BeautifulSoup
import sqlite3
import time
import re

# ================= DB SETUP =================
DB_NAME = "articles.db"

def init_db():
    conn = sqlite3.connect(DB_NAME)
    cur = conn.cursor()
    cur.execute('''
        CREATE TABLE IF NOT EXISTS articles (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            url TEXT,
            title TEXT,
            content TEXT,
            matched_keywords TEXT,
            crawled_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        )
    ''')
    conn.commit()
    conn.close()

# ================== KEYWORDS =================
KEYWORDS = [
    r"AI",
    r"machine learning",
    r"deep learning",
    r"data",
    r"robot",
    r"automation"
]

# ================== WORKER ====================
class CrawlerWorker(threading.Thread):
    def __init__(self, q, name):
        super().__init__(daemon=True)
        self.q = q
        self.name = name

    def run(self):
        while True:
            url = self.q.get()
            if url is None:
                self.q.task_done()
                break
            try:
                self.crawl(url)
            except Exception as e:
                print(f"[{self.name}] Error on {url}: {e}")
            finally:
                self.q.task_done()

    def crawl(self, url):
        print(f"[{self.name}] Crawling {url}")
        resp = requests.get(url, timeout=10)
        if resp.status_code != 200:
            print(f"[{self.name}] Failed to get {url}")
            return

        soup = BeautifulSoup(resp.text, "html.parser")
        title = soup.title.text.strip() if soup.title else "(no title)"
        content = " ".join([p.get_text() for p in soup.find_all("p")])
        matches = []
        for kw in KEYWORDS:
            if re.search(kw, content, re.IGNORECASE):
                matches.append(kw)
        
        if matches:
            conn = sqlite3.connect(DB_NAME)
            cur = conn.cursor()
            cur.execute(
                "INSERT INTO articles (url, title, content, matched_keywords) VALUES (?, ?, ?, ?)",
                (url, title, content[:500], ",".join(matches))
            )
            conn.commit()
            conn.close()
            print(f"[{self.name}] Saved {title}")

# =============== URL PROVIDER ================

def load_seed_urls():
    return [
        "https://edition.cnn.com",
        "https://bbc.com",
        "https://www.reuters.com",
        "https://www.nytimes.com",
        "https://www.bloomberg.com"
    ]

# =============== SCHEDULER ===================
def scheduler_loop(q):
    while True:
        seed_urls = load_seed_urls()
        for url in seed_urls:
            q.put(url)
        print("[Scheduler] Seeds pushed.")
        time.sleep(3600)  # 1 hour

# =============== MAIN ========================

def main():
    init_db()
    q = queue.Queue()
    
    # Start workers
    workers = []
    for i in range(10):
        w = CrawlerWorker(q, name=f"Worker-{i}")
        w.start()
        workers.append(w)

    # Start scheduler
    sched_thread = threading.Thread(target=scheduler_loop, args=(q,), daemon=True)
    sched_thread.start()

    try:
        while True:
            time.sleep(5)
    except KeyboardInterrupt:
        print("[Main] Shutting down...")
        for _ in workers:
            q.put(None)
        for w in workers:
            w.join()

if __name__ == "__main__":
    main()

# =======================
# 추가 코드 (테스트, 유닛테스트, DB 조회 기능 등) 이어붙여서 500줄 이상으로 확장하면 됩니다.
# 예: 크롤링된 결과를 검색하는 인터페이스, 웹서버 API, 결과 리포트 생성기 등 연결 가능
# 필요하다면 그 파트도 이어서 코딩해줄게요!
